"""
Training utilities for RPM-Net

Includes training loop and related functions
"""

import os
import pickle
import numpy as np
import torch
import torch.optim as optim
import matplotlib.pyplot as plt
from .losses import compute_rpm_loss
from .evaluation import evaluate_known_classification, evaluate_unknown_detection
from .data_utils import compute_class_weights


def train_rpm_model(model, train_loader, test_known_loader, validation_unknown_loader, 
                   test_unknown_loaders, idx_to_label, config):
    """è®­ç»ƒRPM+Fisheræ¨¡å‹ï¼Œå¹¶åœ¨æ¯ä¸ªepochè¯„ä¼°éªŒè¯é›†å’Œæµ‹è¯•é›†çš„æ€§èƒ½"""
    print("ğŸš€ å¼€å§‹è®­ç»ƒRPM+Fisheræ¨¡å‹...")
    
    # ä»idx_to_labelåˆ›å»ºlabel_to_idxæ˜ å°„
    label_to_idx = {label: idx for idx, label in idx_to_label.items()}
    
    model = model.to(config.DEVICE)
    optimizer = optim.Adam(model.parameters(), lr=config.LEARNING_RATE)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.EPOCHS)
    
    # è®¡ç®—ç±»åˆ«æƒé‡å¤„ç†ä¸å¹³è¡¡é—®é¢˜
    class_weights = compute_class_weights(train_loader, idx_to_label)
    
    training_history = {
        'total_loss': [],
        'classification_loss': [],
        'margin_loss': [],
        'fisher_loss': [],
        'accuracy': [],
        'validation_aupr_out': [],
        'test_aupr_out': [],
        'test_accuracy': [],
        'test_macro_f1': []
    }
    
    model.train()
    
    for epoch in range(config.EPOCHS):
        epoch_total_loss = 0.0
        epoch_classification_loss = 0.0
        epoch_margin_loss = 0.0
        epoch_fisher_loss = 0.0
        epoch_correct = 0
        epoch_total = 0
        
        for batch_idx, (data, targets) in enumerate(train_loader):
            data, targets = data.to(config.DEVICE), targets.to(config.DEVICE)
            
            # å‰å‘ä¼ æ’­
            embeddings, distances, logits = model(data)
            
            # è®¡ç®—æŸå¤±
            total_loss, classification_loss, margin_loss, fisher_loss = compute_rpm_loss(
                logits, targets, embeddings, model.reciprocal_points, model.margins, 
                config.LAMBDA, config.FISHER_LAMBDA, class_weights
            )
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            total_loss.backward()
            optimizer.step()
            
            # ç»Ÿè®¡
            epoch_total_loss += total_loss.item()
            epoch_classification_loss += classification_loss.item()
            epoch_margin_loss += margin_loss.item()
            epoch_fisher_loss += fisher_loss.item()
            
            # è®¡ç®—å‡†ç¡®ç‡
            _, predicted = torch.max(logits.data, 1)
            epoch_total += targets.size(0)
            epoch_correct += (predicted == targets).sum().item()
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        
        # è®°å½•æ¯è½®çš„å¹³å‡æŒ‡æ ‡
        avg_total_loss = epoch_total_loss / len(train_loader)
        avg_classification_loss = epoch_classification_loss / len(train_loader)
        avg_margin_loss = epoch_margin_loss / len(train_loader)
        avg_fisher_loss = epoch_fisher_loss / len(train_loader)
        accuracy = 100.0 * epoch_correct / epoch_total
        
        training_history['total_loss'].append(avg_total_loss)
        training_history['classification_loss'].append(avg_classification_loss)
        training_history['margin_loss'].append(avg_margin_loss)
        training_history['fisher_loss'].append(avg_fisher_loss)
        training_history['accuracy'].append(accuracy)
        
        # åœ¨æ¯ä¸ªepochè¯„ä¼°æ¨¡å‹æ€§èƒ½
        model.eval()
        
        # è¯„ä¼°å·²çŸ¥ç±»åˆ†ç±»æ€§èƒ½
        known_results = evaluate_known_classification(model, test_known_loader, idx_to_label, config, verbose=False)
        training_history['test_accuracy'].append(known_results['accuracy'])
        training_history['test_macro_f1'].append(known_results['macro_f1'])
        
        # è¯„ä¼°éªŒè¯é›†çš„AUPR-OUT
        validation_metrics = evaluate_unknown_detection(
            model, test_known_loader, validation_unknown_loader, config, unknown_type="validation"
        )
        training_history['validation_aupr_out'].append(validation_metrics['aupr_out'])
        
        # è¯„ä¼°æµ‹è¯•é›†å¹³å‡AUPR-OUT
        test_aupr_outs = []
        unknown_results = {}
        for unknown_class, unknown_loader in test_unknown_loaders.items():
            test_metrics = evaluate_unknown_detection(
                model, test_known_loader, unknown_loader, config, unknown_type="test"
            )
            test_aupr_outs.append(test_metrics['aupr_out'])
            unknown_results[unknown_class] = test_metrics
        
        avg_test_aupr_out = np.mean(test_aupr_outs) if test_aupr_outs else 0.0
        training_history['test_aupr_out'].append(avg_test_aupr_out)
        
        model.train()
        
        # æ‰“å°è®­ç»ƒè¿›åº¦
        print(f"è½®æ¬¡ [{epoch+1}/{config.EPOCHS}]:")
        print(f"  æ€»æŸå¤±: {avg_total_loss:.4f}")
        print(f"  åˆ†ç±»æŸå¤±: {avg_classification_loss:.4f}")
        print(f"  è¾¹é™…æŸå¤±: {avg_margin_loss:.4f}")
        print(f"  FisheræŸå¤±: {avg_fisher_loss:.4f}")
        print(f"  è®­ç»ƒå‡†ç¡®ç‡: {accuracy:.2f}%")
        print(f"  æµ‹è¯•å‡†ç¡®ç‡: {known_results['accuracy']*100:.2f}%")
        print(f"  æµ‹è¯•å®å¹³å‡F1: {known_results['macro_f1']:.4f}")
        print(f"  éªŒè¯é›†AUPR-OUT: {validation_metrics['aupr_out']:.4f}")
        print(f"  æµ‹è¯•é›†å¹³å‡AUPR-OUT: {avg_test_aupr_out:.4f}")
        print(f"  å­¦ä¹ ç‡: {scheduler.get_last_lr()[0]:.6f}")
        
        # ä»ç¬¬10ä¸ªepochå¼€å§‹ï¼Œæ¯ä¸ªepochéƒ½ä¿å­˜ç»“æœ
        if epoch >= 9:
            # åˆ›å»ºå¸¦æœ‰epochç¼–å·çš„æ–‡ä»¶å¤¹
            epoch_dir = os.path.join(config.RESULT_DIR, f"fisher_epoch_{epoch+1}")
            os.makedirs(epoch_dir, exist_ok=True)
            print(f"ğŸ“ åˆ›å»ºç¬¬{epoch+1}è½®ç»“æœç›®å½•: {epoch_dir}")
            
            # ä¿å­˜å½“å‰å®Œæ•´æ¨¡å‹
            complete_model_file = os.path.join(epoch_dir, f'rpm_fisher_model_epoch_{epoch+1}.pth')
            torch.save(model, complete_model_file)
            print(f"ğŸ’¾ å®Œæ•´æ¨¡å‹å·²ä¿å­˜åˆ°: {complete_model_file}")
            
            # ä¿å­˜æ¨¡å‹çŠ¶æ€å­—å…¸
            model_file = os.path.join(epoch_dir, f'rpm_fisher_model_state_epoch_{epoch+1}.pth')
            torch.save({
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'loss': avg_total_loss,
                'config': config.to_dict(),
                'label_to_idx': label_to_idx,
                'idx_to_label': idx_to_label,
                'input_dim': model.feature_extractor[0].in_features,
                'hidden_dim': config.HIDDEN_DIM,
                'embedding_dim': config.EMBEDDING_DIM,
                'num_classes': model.num_classes,
                'gamma': model.gamma
            }, model_file)
            
            # ä¿å­˜å½“å‰è®­ç»ƒå†å²
            history_file = os.path.join(epoch_dir, f'training_history_epoch_{epoch+1}.pkl')
            with open(history_file, 'wb') as f:
                pickle.dump(training_history, f)
            
            # ç»˜åˆ¶å½“å‰è®­ç»ƒæ›²çº¿
            plot_training_curves(training_history, config, save_dir=epoch_dir, epoch_num=epoch+1)
            
            print(f"âœ… ç¬¬{epoch+1}è½®ç»“æœå·²ä¿å­˜åˆ°: {epoch_dir}")
    
    print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ!")
    return model, training_history


def plot_training_curves(training_history, config, save_dir=None, epoch_num=None):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
    if save_dir is None:
        save_dir = config.RESULT_DIR
    
    print("ğŸ“Š ç»˜åˆ¶è®­ç»ƒæ›²çº¿...")
    
    fig, axes = plt.subplots(3, 2, figsize=(14, 12))
    fig.suptitle('RPM+Fisherè®­ç»ƒè¿‡ç¨‹', fontsize=16)
    
    # æ€»æŸå¤±
    axes[0, 0].plot(training_history['total_loss'])
    axes[0, 0].set_title('æ€»æŸå¤±')
    axes[0, 0].set_xlabel('è½®æ¬¡')
    axes[0, 0].set_ylabel('æŸå¤±å€¼')
    axes[0, 0].grid(True)
    
    # åˆ†ç±»æŸå¤±
    axes[0, 1].plot(training_history['classification_loss'], color='orange')
    axes[0, 1].set_title('åˆ†ç±»æŸå¤±')
    axes[0, 1].set_xlabel('è½®æ¬¡')
    axes[0, 1].set_ylabel('æŸå¤±å€¼')
    axes[0, 1].grid(True)
    
    # è¾¹é™…æŸå¤±
    axes[1, 0].plot(training_history['margin_loss'], color='green')
    axes[1, 0].set_title('å¯¹æŠ—è¾¹é™…æŸå¤±')
    axes[1, 0].set_xlabel('è½®æ¬¡')
    axes[1, 0].set_ylabel('æŸå¤±å€¼')
    axes[1, 0].grid(True)
    
    # FisheræŸå¤±
    axes[1, 1].plot(training_history['fisher_loss'], color='purple')
    axes[1, 1].set_title('FisheræŸå¤±')
    axes[1, 1].set_xlabel('è½®æ¬¡')
    axes[1, 1].set_ylabel('æŸå¤±å€¼')
    axes[1, 1].grid(True)
    
    # å‡†ç¡®ç‡
    axes[2, 0].plot(training_history['accuracy'], color='red', label='è®­ç»ƒ')
    if 'test_accuracy' in training_history:
        axes[2, 0].plot(np.array(training_history['test_accuracy']) * 100, color='blue', label='æµ‹è¯•')
    axes[2, 0].set_title('å‡†ç¡®ç‡')
    axes[2, 0].set_xlabel('è½®æ¬¡')
    axes[2, 0].set_ylabel('å‡†ç¡®ç‡ (%)')
    axes[2, 0].legend()
    axes[2, 0].grid(True)
    
    # éªŒè¯é›†å’Œæµ‹è¯•é›†AUPR-OUT
    axes[2, 1].plot(training_history['validation_aupr_out'], color='darkgreen', label='éªŒè¯é›†')
    if 'test_aupr_out' in training_history:
        axes[2, 1].plot(training_history['test_aupr_out'], color='brown', label='æµ‹è¯•é›†')
    axes[2, 1].set_title('AUPR-OUT')
    axes[2, 1].set_xlabel('è½®æ¬¡')
    axes[2, 1].set_ylabel('AUPR-OUT')
    axes[2, 1].legend()
    axes[2, 1].grid(True)
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾ç‰‡
    if epoch_num is not None:
        plot_file = os.path.join(save_dir, f'training_curves_epoch_{epoch_num}.png')
    else:
        plot_file = os.path.join(save_dir, 'training_curves.png')
    
    plt.savefig(plot_file, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"âœ… è®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ°: {plot_file}")
