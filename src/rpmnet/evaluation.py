"""
Evaluation utilities for RPM-Net

Includes functions for evaluating known class classification and unknown class detection
"""

import numpy as np
import torch
import torch.nn.functional as F
from sklearn.metrics import (
    accuracy_score, classification_report, 
    roc_auc_score, average_precision_score, roc_curve
)


def evaluate_known_classification(model, test_known_loader, idx_to_label, config, verbose=True):
    """è¯„ä¼°å·²çŸ¥ç±»åˆ†ç±»æ€§èƒ½"""
    if verbose:
        print("ðŸ“Š è¯„ä¼°å·²çŸ¥ç±»åˆ†ç±»æ€§èƒ½...")
    
    model.eval()
    all_predictions = []
    all_targets = []
    all_probabilities = []
    
    with torch.no_grad():
        for data, targets in test_known_loader:
            data, targets = data.to(config.DEVICE), targets.to(config.DEVICE)
            
            embeddings, distances, logits = model(data)
            probabilities = F.softmax(logits, dim=1)
            _, predicted = torch.max(logits.data, 1)
            
            all_predictions.extend(predicted.cpu().numpy())
            all_targets.extend(targets.cpu().numpy())
            all_probabilities.extend(probabilities.cpu().numpy())
    
    # è½¬æ¢ç´¢å¼•ä¸ºæ ‡ç­¾
    pred_labels = [idx_to_label[idx] for idx in all_predictions]
    true_labels = [idx_to_label[idx] for idx in all_targets]
    
    # è®¡ç®—æŒ‡æ ‡
    accuracy = accuracy_score(true_labels, pred_labels)
    report = classification_report(true_labels, pred_labels, output_dict=True)
    
    # æå–macroæŒ‡æ ‡
    macro_metrics = report['macro avg']
    macro_precision = macro_metrics['precision']
    macro_recall = macro_metrics['recall']
    macro_f1 = macro_metrics['f1-score']
    
    if verbose:
        print(f"âœ… å·²çŸ¥ç±»åˆ†ç±»å‡†ç¡®çŽ‡: {accuracy:.4f}")
        print(f"âœ… å·²çŸ¥ç±»åˆ†ç±»MacroæŒ‡æ ‡:")
        print(f"   Precision (macro): {macro_precision:.4f}")
        print(f"   Recall (macro): {macro_recall:.4f}")
        print(f"   F1-score (macro): {macro_f1:.4f}")
        
        print("\nðŸ“‹ å„ç±»åˆ«è¯¦ç»†æ€§èƒ½:")
        for label in idx_to_label.values():
            if label in report:
                metrics = report[label]
                print(f"  {label}:")
                print(f"    ç²¾ç¡®çŽ‡: {metrics['precision']:.4f}")
                print(f"    å¬å›žçŽ‡: {metrics['recall']:.4f}")
                print(f"    F1åˆ†æ•°: {metrics['f1-score']:.4f}")
    
    return {
        'accuracy': accuracy,
        'classification_report': report,
        'predictions': pred_labels,
        'true_labels': true_labels,
        'probabilities': all_probabilities,
        'macro_precision': macro_precision,
        'macro_recall': macro_recall,
        'macro_f1': macro_f1
    }


def evaluate_unknown_detection(model, test_known_loader, unknown_loader, config, unknown_type="validation"):
    """è¯„ä¼°æœªçŸ¥ç±»æ£€æµ‹æ€§èƒ½ - å¯¹æ¯ä¸ªæœªçŸ¥ç±»åˆ†åˆ«è¯„ä¼°: AUROC, AUPR-IN, AUPR-OUT"""
    model.eval()
    
    # é¦–å…ˆèŽ·å–å·²çŸ¥ç±»çš„ç½®ä¿¡åº¦åˆ†æ•°
    known_scores = []
    
    with torch.no_grad():
        for data, targets in test_known_loader:
            data = data.to(config.DEVICE)
            embeddings, distances, logits = model(data)
            
            # ä½¿ç”¨æœ€å¤§è·ç¦»ä½œä¸ºç½®ä¿¡åº¦åˆ†æ•° (è·ç¦»è¶Šå¤§ï¼Œè¶Šå¯èƒ½æ˜¯å·²çŸ¥ç±»)
            max_distances = torch.max(distances, dim=1)[0]
            known_scores.extend(max_distances.cpu().numpy())
    
    known_scores = np.array(known_scores)
    
    # èŽ·å–æœªçŸ¥ç±»çš„ç½®ä¿¡åº¦åˆ†æ•°
    unknown_scores = []
    
    with torch.no_grad():
        for data, in unknown_loader:
            data = data.to(config.DEVICE)
            embeddings, distances, logits = model(data)
            
            # ä½¿ç”¨æœ€å¤§è·ç¦»ä½œä¸ºç½®ä¿¡åº¦åˆ†æ•°
            max_distances = torch.max(distances, dim=1)[0]
            unknown_scores.extend(max_distances.cpu().numpy())
    
    unknown_scores = np.array(unknown_scores)
    
    # è®¡ç®—æ£€æµ‹æŒ‡æ ‡
    detection_results = compute_detection_metrics(known_scores, unknown_scores)
    
    return detection_results


def evaluate_all_unknown_classes(model, test_known_loader, test_unknown_loaders, config):
    """è¯„ä¼°æ‰€æœ‰æœªçŸ¥ç±»æ£€æµ‹æ€§èƒ½ - å¯¹æ¯ä¸ªæœªçŸ¥ç±»åˆ†åˆ«è¯„ä¼°"""
    print("ðŸ“Š è¯„ä¼°æœªçŸ¥ç±»æ£€æµ‹æ€§èƒ½...")
    
    results = {}
    
    for unknown_class, unknown_loader in test_unknown_loaders.items():
        print(f"  è¯„ä¼°æœªçŸ¥ç±»: {unknown_class}")
        
        detection_results = evaluate_unknown_detection(
            model, test_known_loader, unknown_loader, config, unknown_type="test"
        )
        
        results[unknown_class] = detection_results
        
        print(f"    AUROC: {detection_results['auroc']:.4f}")
        print(f"    AUPR-IN: {detection_results['aupr_in']:.4f}")
        print(f"    AUPR-OUT: {detection_results['aupr_out']:.4f}")
        print(f"    FPR@95%TPR: {detection_results['fpr_95']:.4f}")
    
    return results


def compute_detection_metrics(known_scores, unknown_scores):
    """è®¡ç®—OODæ£€æµ‹æŒ‡æ ‡"""
    # æž„é€ æ ‡ç­¾ (0=å·²çŸ¥, 1=æœªçŸ¥)
    y_true = np.concatenate([
        np.zeros(len(known_scores)),  # å·²çŸ¥ç±»æ ‡è®°ä¸º0
        np.ones(len(unknown_scores))  # æœªçŸ¥ç±»æ ‡è®°ä¸º1
    ])
    
    # æž„é€ åˆ†æ•° (åˆ†æ•°è¶Šé«˜è¶Šå¯èƒ½æ˜¯æœªçŸ¥ç±»)
    # ç”±äºŽæˆ‘ä»¬çš„æ¨¡åž‹è¾“å‡ºçš„æ˜¯"è¶Šå¤§è¶Šåƒå·²çŸ¥ç±»"çš„åˆ†æ•°ï¼Œæ‰€ä»¥è¦å–è´Ÿå·
    y_scores = np.concatenate([-known_scores, -unknown_scores])
    
    # è®¡ç®—AUROC
    auroc = roc_auc_score(y_true, y_scores)
    
    # è®¡ç®—AUPR-IN (å·²çŸ¥ç±»ä¸ºæ­£ä¾‹)
    y_true_in = 1 - y_true  # å·²çŸ¥ç±»ä¸ºæ­£ä¾‹ (1=å·²çŸ¥, 0=æœªçŸ¥)
    aupr_in = average_precision_score(y_true_in, -y_scores)
    
    # è®¡ç®—AUPR-OUT (æœªçŸ¥ç±»ä¸ºæ­£ä¾‹)
    aupr_out = average_precision_score(y_true, y_scores)
    
    # è®¡ç®—FPR@95%TPR
    fpr, tpr, thresholds = roc_curve(y_true, y_scores)
    
    # æ‰¾åˆ°TPR >= 0.95çš„æœ€å°FPR
    valid_indices = tpr >= 0.95
    if np.any(valid_indices):
        fpr_95 = np.min(fpr[valid_indices])
    else:
        fpr_95 = 1.0  # å¦‚æžœæ— æ³•è¾¾åˆ°95% TPRï¼Œåˆ™FPRè®¾ä¸º1
    
    return {
        'auroc': auroc,
        'aupr_in': aupr_in,
        'aupr_out': aupr_out,
        'fpr_95': fpr_95
    }
